{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93545df",
   "metadata": {},
   "source": [
    "# Automatic Stimuli Creation for Degrading Visual Information about Articulation or Mouthing\n",
    "<br>\n",
    "<div align=\"center\">Wim Pouw (wim.pouw@donders.ru.nl) & Annika Schiefner (a.schiefner@uva.nl)>\n",
    "</div>\n",
    "\n",
    "<img src=\"Images/mask_comparison.gif\" alt=\"isolated\" width=\"300\"/>\n",
    "\n",
    "## Info documents\n",
    "\n",
    "<img src=\"Images/envision_banner.png\" alt=\"isolated\" width=\"100\"/>\n",
    "\n",
    "\n",
    "This script uses mediapipe to automatically and dynamically blur parts of the face, now specifically the mouth region. This can be helpful for stimuli generation in the application of sign languages but also spoken languages. See below for a description of this.\n",
    "\n",
    "* location Repository:  https://github.com/WimPouw/StimuliCreationMaskingMouth.ipynb\n",
    "\n",
    "* location Jupyter notebook: https://github.com/WimPouw/AutoVisualDegradArticulationMouthing/blob/main/StimuliCreationMaskingMouth.ipynb\n",
    "\n",
    "Current Github: https://github.com/WimPouw/AutoVisualDegradArticulationMouthing\n",
    "\n",
    "# Citations\n",
    "* Pouw, W., & Schiefner, A. (2025). Masking the mouth region for visual degradation of articulation and mouthing (Version 1.0.0) [Software]. Retrieved from https://github.com/WimPouw/AutoVisualDegradArticulationMouthing\n",
    "\n",
    "\n",
    "\n",
    "## Application for Sign langauages\n",
    "In sign language research, psycholinguists often work with videos of individual signs, akin to working with single spoken words and gestures. These individual signs have different components, each contributing to the information conveyed by the lexical signs. This includes the hands, movements of the body and facial expressions and movements of the mouth.\n",
    "\n",
    "Sign languages, existing in close proximity to hearing communities, often incorporate the movements associated with spoken words in the signs. When producing individual signs, signers may thus produce a mouth movement that looks as though they were saying the word alongside the hand movement. In fact, signers often find it difficult to produce signs with a neutral face and the resulting videos are perceived as less natural. This mean, in turn, that observers may be able to extract information about the lexical item from looking at the mouth, even without looking at the hands. If you want to investigate what information is conveyed by the manual movements without influence from what the mouth is contributing, you therefore need to mask the mouth area. This module provides an approach to doing that, blurring the area around the mouth such that lip reading becomes impossible.\n",
    "\n",
    "Whenever the signers hands now move into the area around the mouth, you need to decide what to do with them. They can be covered by the same mask such that those parts of the hand that are close to the mouth are obscured as well or they can be excluded from the mask so the hand stays visible even around the mouth. As some handshapes are easier to identify and fit into a mask, e.g. a fist is a nice little circle and is therefore easy to track, while shaping the hand like a C leaves an opening between the fingers and the thumb which is more difficult to mask. Therefore, some portions of the mouth may still be visible with this option. The module provides different options, so you get to choose which one you prefer. Try and see for your own items and purpose what works best.\n",
    "\n",
    "## Application for Spoken Languages\n",
    "Perhaps you know the phenomenon of McGurk, where information about lip or tongue movements, i.e., articulatory gestures, influence the sounds you tend to hear. With this code you can easily adjust how much of such information is present.\n",
    "\n",
    "## Use\n",
    "Make sure to install all the packages in requirements.txt. Then move your videos that you want to mask into the input folder. Then run this code, which will loop through all the videos contained in the input folder; and saves all the results in the output folders.\n",
    "\n",
    "Please use, improve and adapt as you see fit.\n",
    "\n",
    "This python notebook runs you through the procedure of taking videos as inputs with a single person in the video, and outputting the 1 outputs of the kinematic timeseries, and optionally masking video with facial, hand, and arm kinematics ovelays.\n",
    "\n",
    "## Additional information backbone of the tool (Mediapipe Holistic Tracking)\n",
    "https://google.github.io/mediapipe/solutions/holistic.html\n",
    "\n",
    "## Citation of mediapipe\n",
    "citation: Lugaresi, C., Tang, J., Nash, H., McClanahan, C., Uboweja, E., Hays, M., ... & Grundmann, M. (2019). Mediapipe: A framework for building perception pipelines. arXiv preprint arXiv:1906.08172.\n",
    "\n",
    "## Modification that is the basis of this tool\n",
    "Our modification of the Mediapipe tool is using the coordinates of mediapipe to determine a region that we mask with a blur. We can change the coordinates for this bounding polygon if we want to blur other regions on the face. Please have a look below for more information about the keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f423ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following folder is set as the output folder where all the pose time series are stored\n",
      "d:\\Research_projects\\AutoVisualDegradArticulationMouthing\\Output_TimeSeries\n",
      "\n",
      " The following folder is set as the output folder for saving the masked videos \n",
      "d:\\Research_projects\\AutoVisualDegradArticulationMouthing\\Output_Videos\n",
      "\n",
      " The following video(s) will be processed for masking: \n",
      "['DOLFIJN.mp4', 'ETEN.mp4', 'NULL.mp4', 'OCHTEND.mp4', 'OLIFANT.mp4', 'RIETJE.mp4']\n",
      "Note that we have the following number of pose keypoints for markers body\n",
      "33\n",
      "\n",
      " Note that we have the following number of pose keypoints for markers hands\n",
      "42\n",
      "\n",
      " Note that we have the following number of pose keypoints for markers face\n",
      "478\n"
     ]
    }
   ],
   "source": [
    "#load in required packages\n",
    "import mediapipe as mp #mediapipe\n",
    "import cv2 #opencv\n",
    "import math #basic operations\n",
    "import numpy as np #basic operations\n",
    "import pandas as pd #data wrangling\n",
    "import csv #csv saving\n",
    "import os #some basic functions for inspecting folder structure etc.\n",
    "\n",
    "#list all videos in input_videofolder\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"./Input_Videos/\" #this is your folder with (all) your video(s)\n",
    "vfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))] #loop through the filenames and collect them in a list\n",
    "#time series output folder\n",
    "inputfol = \"./Input_Videos/\"\n",
    "outputf_mask = \"./Output_Videos/\"\n",
    "outtputf_ts = \"./Output_TimeSeries/\"\n",
    "\n",
    "#check videos to be processed\n",
    "print(\"The following folder is set as the output folder where all the pose time series are stored\")\n",
    "print(os.path.abspath(outtputf_ts))\n",
    "print(\"\\n The following folder is set as the output folder for saving the masked videos \")\n",
    "print(os.path.abspath(outputf_mask))\n",
    "print(\"\\n The following video(s) will be processed for masking: \")\n",
    "print(vfiles)\n",
    "\n",
    "#initialize modules and functions\n",
    "\n",
    "#load in mediapipe modules\n",
    "mp_holistic = mp.solutions.holistic\n",
    "# Import drawing_utils and drawing_styles.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "##################FUNCTIONS AND OTHER VARIABLES\n",
    "#landmarks 33x that are used by Mediapipe (Blazepose)\n",
    "markersbody = ['NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_OUTER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER',\n",
    "          'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', \n",
    "          'RIGHT_ELBOW', 'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', 'RIGHT_INDEX',\n",
    "          'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE',\n",
    "          'LEFT_HEEL', 'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
    "\n",
    "markershands = ['LEFT_WRIST', 'LEFT_THUMB_CMC', 'LEFT_THUMB_MCP', 'LEFT_THUMB_IP', 'LEFT_THUMB_TIP', 'LEFT_INDEX_FINGER_MCP',\n",
    "              'LEFT_INDEX_FINGER_PIP', 'LEFT_INDEX_FINGER_DIP', 'LEFT_INDEX_FINGER_TIP', 'LEFT_MIDDLE_FINGER_MCP', \n",
    "               'LEFT_MIDDLE_FINGER_PIP', 'LEFT_MIDDLE_FINGER_DIP', 'LEFT_MIDDLE_FINGER_TIP', 'LEFT_RING_FINGER_MCP', \n",
    "               'LEFT_RING_FINGER_PIP', 'LEFT_RING_FINGER_DIP', 'LEFT_RING_FINGER_TIP', 'LEFT_PINKY_FINGER_MCP', \n",
    "               'LEFT_PINKY_FINGER_PIP', 'LEFT_PINKY_FINGER_DIP', 'LEFT_PINKY_FINGER_TIP',\n",
    "              'RIGHT_WRIST', 'RIGHT_THUMB_CMC', 'RIGHT_THUMB_MCP', 'RIGHT_THUMB_IP', 'RIGHT_THUMB_TIP', 'RIGHT_INDEX_FINGER_MCP',\n",
    "              'RIGHT_INDEX_FINGER_PIP', 'RIGHT_INDEX_FINGER_DIP', 'RIGHT_INDEX_FINGER_TIP', 'RIGHT_MIDDLE_FINGER_MCP', \n",
    "               'RIGHT_MIDDLE_FINGER_PIP', 'RIGHT_MIDDLE_FINGER_DIP', 'RIGHT_MIDDLE_FINGER_TIP', 'RIGHT_RING_FINGER_MCP', \n",
    "               'RIGHT_RING_FINGER_PIP', 'RIGHT_RING_FINGER_DIP', 'RIGHT_RING_FINGER_TIP', 'RIGHT_PINKY_FINGER_MCP', \n",
    "               'RIGHT_PINKY_FINGER_PIP', 'RIGHT_PINKY_FINGER_DIP', 'RIGHT_PINKY_FINGER_TIP']\n",
    "facemarks = [str(x) for x in range(478)] #there are 478 points for the face mesh (see google holistic face mesh info for landmarks)\n",
    "\n",
    "print(\"Note that we have the following number of pose keypoints for markers body\")\n",
    "print(len(markersbody))\n",
    "\n",
    "print(\"\\n Note that we have the following number of pose keypoints for markers hands\")\n",
    "print(len(markershands))\n",
    "\n",
    "print(\"\\n Note that we have the following number of pose keypoints for markers face\")\n",
    "print(len(facemarks ))\n",
    "\n",
    "#set up the column names and objects for the time series data (add time as the first variable)\n",
    "markerxyzbody = ['time']\n",
    "markerxyzhands = ['time']\n",
    "markerxyzface = ['time']\n",
    "\n",
    "for mark in markersbody:\n",
    "    for pos in ['X', 'Y', 'Z', 'visibility']: #for markers of the body you also have a visibility reliability score\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzbody.append(nm)\n",
    "for mark in markershands:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzhands.append(nm)\n",
    "for mark in facemarks:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzface.append(nm)\n",
    "\n",
    "#check if there are numbers in a string\n",
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "#take some google classification object and convert it into a string\n",
    "def makegoginto_str(gogobj):\n",
    "    gogobj = str(gogobj).strip(\"[]\")\n",
    "    gogobj = gogobj.split(\"\\n\")\n",
    "    return(gogobj[:-1]) #ignore last element as this has nothing\n",
    "\n",
    "#make the stringifyd position traces into clean numerical values\n",
    "def listpostions(newsamplemarks):\n",
    "    newsamplemarks = makegoginto_str(newsamplemarks)\n",
    "    tracking_p = []\n",
    "    for value in newsamplemarks:\n",
    "        if num_there(value):\n",
    "            stripped = value.split(':', 1)[1]\n",
    "            stripped = stripped.strip() #remove spaces in the string if present\n",
    "            tracking_p.append(stripped) #add to this list  \n",
    "    return(tracking_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9783d",
   "metadata": {},
   "source": [
    "## Main procedure\n",
    "\n",
    "Below we apply a blur mask, with some blurring value (how many pixels are mixed) and an opacity value (how much is the original image blocked), for particular position values of the face mask. In the images folder you will find this image, where if you would zoom in will have numbers:\n",
    "\n",
    "<img src=\"Images/keypoints_holistic_face.png\" alt=\"isolated\" width=\"600\"/>\n",
    "\n",
    "\n",
    "# Mouth landmarks for masking\n",
    "Now the position landmark values for the mouth region we have identified as follows.\n",
    "\n",
    "MOUTH_LANDMARKS = [192, 206, 2, 426, 436, 434, 431, 211]\n",
    "\n",
    "Note that nothing is stopping you to draw other polygons that cover some other area of the face!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f5e7270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will now process video:\n",
      "DOLFIJN.mp4\n",
      "This is video number 0 of 6 videos in total\n",
      "We will now process video:\n",
      "ETEN.mp4\n",
      "This is video number 1 of 6 videos in total\n",
      "We will now process video:\n",
      "NULL.mp4\n",
      "This is video number 2 of 6 videos in total\n",
      "We will now process video:\n",
      "OCHTEND.mp4\n",
      "This is video number 3 of 6 videos in total\n",
      "We will now process video:\n",
      "OLIFANT.mp4\n",
      "This is video number 4 of 6 videos in total\n",
      "We will now process video:\n",
      "RIETJE.mp4\n",
      "This is video number 5 of 6 videos in total\n",
      "Done with processing all folders; go look in your output folders!\n"
     ]
    }
   ],
   "source": [
    "# do you want to apply masking?\n",
    "masking = True\n",
    "blur_kernel_size = 111  # Adjust this value to change blur intensity\n",
    "opacity = 1  # 0.0 is fully transparent, 1.0 is fully opaque\n",
    "\n",
    "# Mouth landmarks for masking\n",
    "MOUTH_LANDMARKS = [192, 206, 2, 426, 436, 434, 431, 211]\n",
    "\n",
    "# We will now loop over all the videos that are present in the video file\n",
    "for vidf in vfiles:\n",
    "    print(\"We will now process video:\")\n",
    "    print(vidf)\n",
    "    print(\"This is video number \" + str(vfiles.index(vidf))+ \" of \" + str(len(vfiles)) + \" videos in total\")\n",
    "    \n",
    "    videoname = vidf\n",
    "    videoloc = inputfol + videoname\n",
    "    capture = cv2.VideoCapture(videoloc)\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(outputf_mask+'handv3_'+videoname, fourcc, \n",
    "                         fps = samplerate, frameSize = (int(frameWidth), int(frameHeight)))\n",
    "\n",
    "    time = 0\n",
    "    tsbody = [markerxyzbody]\n",
    "    tshands = [markerxyzhands]\n",
    "    tsface = [markerxyzface]\n",
    "    \n",
    "    with mp_holistic.Holistic(\n",
    "            static_image_mode=False, enable_segmentation=True, refine_face_landmarks=True) as holistic:\n",
    "        while (True):\n",
    "            ret, image = capture.read()\n",
    "            if ret == True:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(image)\n",
    "                \n",
    "                h, w, c = image.shape\n",
    "                if np.all(results.face_landmarks) != None:\n",
    "                    original_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    if masking:\n",
    "                        # Create mask for mouth area\n",
    "                        mouth_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                        landmarks = results.face_landmarks.landmark\n",
    "                        \n",
    "                        # Get mouth area points\n",
    "                        mouth_points = np.array([(int(landmarks[idx].x * w), int(landmarks[idx].y * h)) \n",
    "                                               for idx in MOUTH_LANDMARKS], dtype=np.int32)\n",
    "                        \n",
    "                        # Fill mouth polygon\n",
    "                        cv2.fillPoly(mouth_mask, [mouth_points], 255)\n",
    "                        \n",
    "                        # Create hand mask\n",
    "                        hand_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                        \n",
    "                        # Draw hands on the mask\n",
    "                        if results.left_hand_landmarks:\n",
    "                            hand_points = []\n",
    "                            for landmark in results.left_hand_landmarks.landmark:\n",
    "                                x = int(landmark.x * w)\n",
    "                                y = int(landmark.y * h)\n",
    "                                hand_points.append((x, y))\n",
    "                            if len(hand_points) > 0:\n",
    "                                hull = cv2.convexHull(np.array(hand_points))\n",
    "                                cv2.fillConvexPoly(hand_mask, hull, 255)\n",
    "                                \n",
    "                        if results.right_hand_landmarks:\n",
    "                            hand_points = []\n",
    "                            for landmark in results.right_hand_landmarks.landmark:\n",
    "                                x = int(landmark.x * w)\n",
    "                                y = int(landmark.y * h)\n",
    "                                hand_points.append((x, y))\n",
    "                            if len(hand_points) > 0:\n",
    "                                hull = cv2.convexHull(np.array(hand_points))\n",
    "                                cv2.fillConvexPoly(hand_mask, hull, 255)\n",
    "                        \n",
    "                        # Create a more precise hand mask with minimal dilation\n",
    "                        kernel = np.ones((5,5), np.uint8)\n",
    "                        hand_mask = cv2.dilate(hand_mask, kernel, iterations=1)\n",
    "                        \n",
    "                        # Smooth the edges of the hand mask\n",
    "                        hand_mask = cv2.GaussianBlur(hand_mask, (3,3), 0)\n",
    "                        _, hand_mask = cv2.threshold(hand_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "                        \n",
    "                        # First apply the mouth blur\n",
    "                        blur_region = cv2.GaussianBlur(original_image, (blur_kernel_size, blur_kernel_size), 0)\n",
    "                        mask_3channel = cv2.cvtColor(mouth_mask, cv2.COLOR_GRAY2BGR) / 255.0\n",
    "                        blurred_image = (original_image * (1 - mask_3channel * opacity) + \n",
    "                                       blur_region * (mask_3channel * opacity)).astype(np.uint8)\n",
    "                        \n",
    "                        # Then overlay the hands\n",
    "                        hand_mask_3channel = cv2.cvtColor(hand_mask, cv2.COLOR_GRAY2BGR) / 255.0\n",
    "                        original_image = (blurred_image * (1 - hand_mask_3channel) + \n",
    "                                        original_image * hand_mask_3channel).astype(np.uint8)\n",
    "                    \n",
    "                    # Save time series data\n",
    "                    samplebody = listpostions(results.pose_landmarks)\n",
    "                    samplehands = listpostions([results.left_hand_landmarks, results.right_hand_landmarks])\n",
    "                    sampleface = listpostions(results.face_landmarks)\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "                    tsbody.append(samplebody)\n",
    "                    tshands.append(samplehands)\n",
    "                    tsface.append(sampleface)\n",
    "                    \n",
    "                if np.all(results.face_landmarks) == None:\n",
    "                    original_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    samplebody = [np.nan for x in range(len(markerxyzbody)-1)]\n",
    "                    samplehands = [np.nan for x in range(len(markerxyzhands)-1)]\n",
    "                    sampleface = [np.nan for x in range(len(markerxyzface)-1)]\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "                    tsbody.append(samplebody)\n",
    "                    tshands.append(samplehands)\n",
    "                    tsface.append(sampleface)\n",
    "                \n",
    "                cv2.imshow(\"resizedimage\", original_image)\n",
    "                out.write(original_image)\n",
    "                time = time+(1000/samplerate)\n",
    "                \n",
    "            if cv2.waitKey(1) == 27:\n",
    "                break\n",
    "            if ret == False:\n",
    "                break\n",
    "\n",
    "    out.release()\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Write CSV files\n",
    "    filebody = open(outtputf_ts + vidf[:-4]+'_body.csv', 'w+', newline ='')\n",
    "    with filebody:    \n",
    "        write = csv.writer(filebody)\n",
    "        write.writerows(tsbody)\n",
    "        \n",
    "    filehands = open(outtputf_ts + vidf[:-4]+'_hands.csv', 'w+', newline ='')\n",
    "    with filehands:\n",
    "        write = csv.writer(filehands)\n",
    "        write.writerows(tshands)\n",
    "        \n",
    "    fileface = open(outtputf_ts + vidf[:-4]+'_face.csv', 'w+', newline ='')\n",
    "    with fileface:    \n",
    "        write = csv.writer(fileface)\n",
    "        write.writerows(tsface)\n",
    "\n",
    "print(\"Done with processing all folders; go look in your output folders!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796ebc1",
   "metadata": {},
   "source": [
    "# Making a stimuli set\n",
    "Now we could also make a stimuli set where we iterate over different levels in which visual information is blocked. We also now isolate automatically the head of the persons, in case we only want to show the head (we choose a fixed bounding box on this based on median values of 30 frames). You can undo headzoom, if you want to keep the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec6d78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video 1/6: DOLFIJN.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (417, 181, 597, 388)\n",
      "Processing video 2/6: ETEN.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (431, 203, 609, 414)\n",
      "Processing video 3/6: NULL.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (437, 187, 615, 393)\n",
      "Processing video 4/6: OCHTEND.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (479, 197, 651, 408)\n",
      "Processing video 5/6: OLIFANT.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (406, 195, 584, 400)\n",
      "Processing video 6/6: RIETJE.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (469, 242, 636, 437)\n",
      "Processing video 1/6: DOLFIJN.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (417, 181, 597, 388)\n",
      "Processing video 2/6: ETEN.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (431, 203, 609, 414)\n",
      "Processing video 3/6: NULL.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (437, 187, 615, 393)\n",
      "Processing video 4/6: OCHTEND.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (479, 197, 651, 408)\n",
      "Processing video 5/6: OLIFANT.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (406, 195, 584, 400)\n",
      "Processing video 6/6: RIETJE.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (469, 242, 636, 437)\n",
      "Processing video 1/6: DOLFIJN.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (417, 181, 597, 388)\n",
      "Processing video 2/6: ETEN.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (431, 203, 609, 414)\n",
      "Processing video 3/6: NULL.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (437, 187, 615, 393)\n",
      "Processing video 4/6: OCHTEND.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (479, 197, 651, 408)\n",
      "Processing video 5/6: OLIFANT.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (406, 195, 584, 400)\n",
      "Processing video 6/6: RIETJE.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (469, 242, 636, 437)\n",
      "Processing video 1/6: DOLFIJN.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (417, 181, 597, 388)\n",
      "Processing video 2/6: ETEN.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (431, 203, 609, 414)\n",
      "Processing video 3/6: NULL.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (437, 187, 615, 393)\n",
      "Processing video 4/6: OCHTEND.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (479, 197, 651, 408)\n",
      "Processing video 5/6: OLIFANT.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (406, 195, 584, 400)\n",
      "Processing video 6/6: RIETJE.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (469, 242, 636, 437)\n",
      "Processing video 1/6: DOLFIJN.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (417, 181, 597, 388)\n",
      "Processing video 2/6: ETEN.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (431, 203, 609, 414)\n",
      "Processing video 3/6: NULL.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (437, 187, 615, 393)\n",
      "Processing video 4/6: OCHTEND.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (479, 197, 651, 408)\n",
      "Processing video 5/6: OLIFANT.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (406, 195, 584, 400)\n",
      "Processing video 6/6: RIETJE.mp4\n",
      "Calculating stable head bounds...\n",
      "Head bounds calculated: (469, 242, 636, 437)\n",
      "Done with processing all folders; go look in your output folders!\n"
     ]
    }
   ],
   "source": [
    "# Now make a stimuli creation pipeline\n",
    "blur_kernel_size = 111  # Adjust this value to change blur intensity\n",
    "opacitylist = [0, 0.25, 0.50, 0.75, 1]  # 0.0 is fully transparent, 1.0 is fully opaque\n",
    "headzoom = True\n",
    "masking = True\n",
    "MOUTH_LANDMARKS = [192, 206, 2, 426, 436, 434, 431, 211]\n",
    "\n",
    "def get_head_bounds(landmarks, w, h, padding=0.2):\n",
    "    \"\"\"Calculate head bounding box with padding\"\"\"\n",
    "    face_coords = np.array([(landmark.x * w, landmark.y * h) for landmark in landmarks])\n",
    "    min_x, min_y = np.min(face_coords, axis=0)\n",
    "    max_x, max_y = np.max(face_coords, axis=0)\n",
    "    \n",
    "    # Add padding\n",
    "    width = max_x - min_x\n",
    "    height = max_y - min_y\n",
    "    pad_x = width * padding\n",
    "    pad_y = height * padding\n",
    "    \n",
    "    # Ensure bounds are within image\n",
    "    x1 = max(0, int(min_x - pad_x))\n",
    "    y1 = max(0, int(min_y - pad_y))\n",
    "    x2 = min(w, int(max_x + pad_x))\n",
    "    y2 = min(h, int(max_y + pad_y))\n",
    "    \n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "# Process each video with different opacities\n",
    "for opacity in opacitylist:\n",
    "    for vidf in vfiles:\n",
    "        print(f\"Processing video {vfiles.index(vidf) + 1}/{len(vfiles)}: {vidf}\")\n",
    "        \n",
    "        videoloc = inputfol + vidf\n",
    "        capture = cv2.VideoCapture(videoloc)\n",
    "        frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "        frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "        samplerate = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        # Calculate stable head bounds if head zoom is enabled\n",
    "        head_bounds = None\n",
    "        if headzoom:\n",
    "            print(\"Calculating stable head bounds...\")\n",
    "            temp_capture = cv2.VideoCapture(videoloc)\n",
    "            bounds_list = []\n",
    "            frame_count = 0\n",
    "            \n",
    "            with mp_holistic.Holistic(static_image_mode=False, enable_segmentation=True, refine_face_landmarks=True) as holistic:\n",
    "                while frame_count < 30:  # Check first 30 frames\n",
    "                    ret, image = temp_capture.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    \n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    results = holistic.process(image)\n",
    "                    \n",
    "                    if results.face_landmarks:\n",
    "                        h, w, _ = image.shape\n",
    "                        bounds = get_head_bounds(results.face_landmarks.landmark, w, h)\n",
    "                        bounds_list.append(bounds)\n",
    "                        frame_count += 1\n",
    "            \n",
    "            temp_capture.release()\n",
    "            \n",
    "            if bounds_list:\n",
    "                bounds_array = np.array(bounds_list)\n",
    "                head_bounds = tuple(map(int, np.median(bounds_array, axis=0)))\n",
    "                x1, y1, x2, y2 = head_bounds\n",
    "                frameWidth = x2 - x1\n",
    "                frameHeight = y2 - y1\n",
    "                print(f\"Head bounds calculated: {head_bounds}\")\n",
    "            else:\n",
    "                print(\"Warning: Could not detect face in initial frames\")\n",
    "                headzoom = False\n",
    "\n",
    "        # Set up video writer with appropriate dimensions\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "        if headzoom and head_bounds is not None:\n",
    "            out = cv2.VideoWriter(\n",
    "                outputf_mask + f'opacity{opacity}_headcrop_{vidf}', \n",
    "                fourcc, fps=samplerate, \n",
    "                frameSize=(int(frameWidth), int(frameHeight))\n",
    "            )\n",
    "        else:\n",
    "            out = cv2.VideoWriter(\n",
    "                outputf_mask + f'opacity{opacity}_{vidf}', \n",
    "                fourcc, fps=samplerate, \n",
    "                frameSize=(int(frameWidth), int(frameHeight))\n",
    "            )\n",
    "\n",
    "        # Initialize time series storage\n",
    "        time = 0\n",
    "        tsbody = [markerxyzbody]\n",
    "        tshands = [markerxyzhands]\n",
    "        tsface = [markerxyzface]\n",
    "        \n",
    "        # Process video frames\n",
    "        with mp_holistic.Holistic(static_image_mode=False, enable_segmentation=True, refine_face_landmarks=True) as holistic:\n",
    "            while True:\n",
    "                ret, image = capture.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                    \n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(image)\n",
    "                \n",
    "                original_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                if results.face_landmarks:\n",
    "                    h, w, c = original_image.shape\n",
    "                    \n",
    "                    if masking:\n",
    "                        # Create mouth mask\n",
    "                        mouth_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                        landmarks = results.face_landmarks.landmark\n",
    "                        mouth_points = np.array([(int(landmarks[idx].x * w), int(landmarks[idx].y * h)) \n",
    "                                            for idx in MOUTH_LANDMARKS], dtype=np.int32)\n",
    "                        cv2.fillPoly(mouth_mask, [mouth_points], 255)\n",
    "                        \n",
    "                        # Create hand mask\n",
    "                        hand_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                        \n",
    "                        # Add hands to mask\n",
    "                        if results.left_hand_landmarks:\n",
    "                            hand_points = []\n",
    "                            for landmark in results.left_hand_landmarks.landmark:\n",
    "                                x = int(landmark.x * w)\n",
    "                                y = int(landmark.y * h)\n",
    "                                hand_points.append((x, y))\n",
    "                            if hand_points:\n",
    "                                hull = cv2.convexHull(np.array(hand_points))\n",
    "                                cv2.fillConvexPoly(hand_mask, hull, 255)\n",
    "                                \n",
    "                        if results.right_hand_landmarks:\n",
    "                            hand_points = []\n",
    "                            for landmark in results.right_hand_landmarks.landmark:\n",
    "                                x = int(landmark.x * w)\n",
    "                                y = int(landmark.y * h)\n",
    "                                hand_points.append((x, y))\n",
    "                            if hand_points:\n",
    "                                hull = cv2.convexHull(np.array(hand_points))\n",
    "                                cv2.fillConvexPoly(hand_mask, hull, 255)\n",
    "                        \n",
    "                        # Smooth hand mask edges\n",
    "                        kernel = np.ones((5,5), np.uint8)\n",
    "                        hand_mask = cv2.dilate(hand_mask, kernel, iterations=1)\n",
    "                        hand_mask = cv2.GaussianBlur(hand_mask, (3,3), 0)\n",
    "                        _, hand_mask = cv2.threshold(hand_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "                        \n",
    "                        # Apply mouth blur\n",
    "                        blur_region = cv2.GaussianBlur(original_image, (blur_kernel_size, blur_kernel_size), 0)\n",
    "                        mask_3channel = cv2.cvtColor(mouth_mask, cv2.COLOR_GRAY2BGR) / 255.0\n",
    "                        blurred_image = (original_image * (1 - mask_3channel * opacity) + \n",
    "                                    blur_region * (mask_3channel * opacity)).astype(np.uint8)\n",
    "                        \n",
    "                        # Overlay hands\n",
    "                        hand_mask_3channel = cv2.cvtColor(hand_mask, cv2.COLOR_GRAY2BGR) / 255.0\n",
    "                        original_image = (blurred_image * (1 - hand_mask_3channel) + \n",
    "                                        original_image * hand_mask_3channel).astype(np.uint8)\n",
    "                    \n",
    "                    # Save landmarks data\n",
    "                    samplebody = listpostions(results.pose_landmarks)\n",
    "                    samplehands = listpostions([results.left_hand_landmarks, results.right_hand_landmarks])\n",
    "                    sampleface = listpostions(results.face_landmarks)\n",
    "                    \n",
    "                else:\n",
    "                    # No face detected\n",
    "                    samplebody = [np.nan for x in range(len(markerxyzbody)-1)]\n",
    "                    samplehands = [np.nan for x in range(len(markerxyzhands)-1)]\n",
    "                    sampleface = [np.nan for x in range(len(markerxyzface)-1)]\n",
    "                \n",
    "                # Apply head zooming after all processing\n",
    "                if headzoom and head_bounds is not None:\n",
    "                    x1, y1, x2, y2 = head_bounds\n",
    "                    original_image = original_image[y1:y2, x1:x2]\n",
    "                \n",
    "                # Add time to samples and append to time series\n",
    "                samplebody.insert(0, time)\n",
    "                samplehands.insert(0, time)\n",
    "                sampleface.insert(0, time)\n",
    "                tsbody.append(samplebody)\n",
    "                tshands.append(samplehands)\n",
    "                tsface.append(sampleface)\n",
    "                \n",
    "                cv2.imshow(\"resizedimage\", original_image)\n",
    "                out.write(original_image)\n",
    "                time = time + (1000/samplerate)\n",
    "                \n",
    "                if cv2.waitKey(1) == 27:\n",
    "                    break\n",
    "\n",
    "        # Clean up video resources\n",
    "        out.release()\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Write CSV files only if they don't exist\n",
    "        body_path = outtputf_ts + vidf[:-4] + '_body.csv'\n",
    "        hands_path = outtputf_ts + vidf[:-4] + '_hands.csv'\n",
    "        face_path = outtputf_ts + vidf[:-4] + '_face.csv'\n",
    "\n",
    "        if not os.path.exists(body_path):\n",
    "            with open(body_path, 'w+', newline='') as filebody:\n",
    "                write = csv.writer(filebody)\n",
    "                write.writerows(tsbody)\n",
    "                print(f\"Saved body data to {body_path}\")\n",
    "\n",
    "        if not os.path.exists(hands_path):\n",
    "            with open(hands_path, 'w+', newline='') as filehands:\n",
    "                write = csv.writer(filehands)\n",
    "                write.writerows(tshands)\n",
    "                print(f\"Saved hands data to {hands_path}\")\n",
    "\n",
    "        if not os.path.exists(face_path):\n",
    "            with open(face_path, 'w+', newline='') as fileface:\n",
    "                write = csv.writer(fileface)\n",
    "                write.writerows(tsface)\n",
    "                print(f\"Saved face data to {face_path}\")\n",
    "\n",
    "print(\"Done with processing all folders; go look in your output folders!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5dcb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Create a gif with all opacities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32773706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created GIF: mask_comparison.gif\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "\n",
    "def create_comparison_gif(output_folder, output_name=\"comparison.gif\", grid_size=(4, 4), duration=100):\n",
    "    # Get all video files for each opacity\n",
    "    video_files = {}\n",
    "    for opacity in [0, 0.50, 1]:\n",
    "        pattern = os.path.join(output_folder, f'opacity{opacity}_*.mp4')\n",
    "        video_files[opacity] = glob.glob(pattern)\n",
    "    \n",
    "    # Randomly select videos for each grid position\n",
    "    selected_videos = []\n",
    "    for _ in range(grid_size[0] * grid_size[1]):\n",
    "        opacity = random.choice(list(video_files.keys()))\n",
    "        if video_files[opacity]:  # If there are videos available for this opacity\n",
    "            video = random.choice(video_files[opacity])\n",
    "            selected_videos.append((opacity, video))\n",
    "    \n",
    "    # Open all videos\n",
    "    video_captures = []\n",
    "    for _, video_path in selected_videos:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        video_captures.append(cap)\n",
    "    \n",
    "    # Get video properties\n",
    "    sample_cap = video_captures[0]\n",
    "    frame_width = int(sample_cap.get(cv2.CAP_PROP_FRAME_WIDTH))*2\n",
    "    frame_height = int(sample_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))*2\n",
    "    fps = int(sample_cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Calculate dimensions for grid cells\n",
    "    cell_width = frame_width // grid_size[1]\n",
    "    cell_height = frame_height // grid_size[0]\n",
    "    \n",
    "    # Create list to store frames for GIF\n",
    "    gif_frames = []\n",
    "    \n",
    "    while True:\n",
    "        # Read frames from all videos\n",
    "        frames = []\n",
    "        all_read = True\n",
    "        \n",
    "        for cap in video_captures:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                all_read = False\n",
    "                break\n",
    "            # Resize frame to fit grid cell\n",
    "            frame = cv2.resize(frame, (cell_width, cell_height))\n",
    "            frames.append(frame)\n",
    "        \n",
    "        if not all_read:\n",
    "            break\n",
    "        \n",
    "        # Create grid frame\n",
    "        grid_frame = np.zeros((cell_height * grid_size[0], \n",
    "                             cell_width * grid_size[1], \n",
    "                             3), dtype=np.uint8)\n",
    "        \n",
    "        # Fill grid with frames\n",
    "        for idx, frame in enumerate(frames):\n",
    "            i = idx // grid_size[1]\n",
    "            j = idx % grid_size[1]\n",
    "            grid_frame[i*cell_height:(i+1)*cell_height, \n",
    "                      j*cell_width:(j+1)*cell_width] = frame\n",
    "        \n",
    "        # Add opacity labels\n",
    "        for idx, (opacity, _) in enumerate(selected_videos):\n",
    "            i = idx // grid_size[1]\n",
    "            j = idx % grid_size[1]\n",
    "                   \n",
    "        # Convert BGR to RGB for PIL\n",
    "        grid_frame_rgb = cv2.cvtColor(grid_frame, cv2.COLOR_BGR2RGB)\n",
    "        gif_frames.append(Image.fromarray(grid_frame_rgb))\n",
    "    \n",
    "    # Release video captures\n",
    "    for cap in video_captures:\n",
    "        cap.release()\n",
    "    \n",
    "    # Save as GIF\n",
    "    if gif_frames:\n",
    "        gif_frames[0].save(\n",
    "            os.path.join(output_folder, output_name),\n",
    "            save_all=True,\n",
    "            append_images=gif_frames[1:],\n",
    "            duration=duration,  # milliseconds per frame\n",
    "            loop=0\n",
    "        )\n",
    "        print(f\"Created GIF: {output_name}\")\n",
    "    else:\n",
    "        print(\"No frames were processed. Check if videos are readable.\")\n",
    "\n",
    "# Usage\n",
    "output_folder = outputf_mask  # Use your output folder path\n",
    "create_comparison_gif(output_folder, \n",
    "                     output_name=\"mask_comparison.gif\",\n",
    "                     grid_size=(4, 4),\n",
    "                     duration=100)  # Adjust duration (ms) as needed (fps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
